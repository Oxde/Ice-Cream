{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662ee0f2",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "\n",
    "**Goal**: Clean individual datasets and save them separately for inspection.\n",
    "\n",
    "**What we do:**\n",
    "1. Load raw Excel files\n",
    "2. Basic cleaning (dates, remove empty columns, handle missing values)\n",
    "3. **Filter to 2022+ only** (as discussed with client)\n",
    "4. **FIXED: Proper date parsing** for search data (DD-MM-YYYY string format)\n",
    "5. Special processing for promo data (pivot and classification)\n",
    "6. Save each dataset as clean CSV\n",
    "7. **NO UNIFICATION YET** - we inspect each dataset first\n",
    "\n",
    "**What we DON'T do:**\n",
    "- Merge datasets\n",
    "- Complex transformations beyond promo processing\n",
    "- Weekly aggregation (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17df4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Basic Data Cleaning - Individual Datasets\n",
      "==================================================\n",
      "ğŸš¨ FIXED: Search data date parsing (DD-MM-YYYY string format)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ§¹ Basic Data Cleaning - Individual Datasets\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš¨ FIXED: Search data date parsing (DD-MM-YYYY string format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c467e6fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Excel files:\n",
      "  ğŸ“ email.xlsx\n",
      "  ğŸ“ sales.xlsx\n",
      "  ğŸ“ tv_promo.xlsx\n",
      "  ğŸ“ tv_branding.xlsx\n",
      "  ğŸ“ radio_local.xlsx\n",
      "  ğŸ“ social.xlsx\n",
      "  ğŸ“ promo.xlsx\n",
      "  ğŸ“ radio_national.xlsx\n",
      "  ğŸ“ search.xlsx\n",
      "  ğŸ“ ooh.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load all raw Excel files\n",
    "raw_dir = '../data/raw'\n",
    "excel_files = glob.glob(os.path.join(raw_dir, '*.xlsx'))\n",
    "\n",
    "print(f\"Found {len(excel_files)} Excel files:\")\n",
    "for file in excel_files:\n",
    "    print(f\"  ğŸ“ {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0974cc44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 2: FIXED Basic cleaning function with proper date handling\n",
    "def basic_clean_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Basic cleaning for individual dataset:\n",
    "    - Load Excel file\n",
    "    - Standardize date column names\n",
    "    - FIXED: Convert dates to datetime (special handling for search data)\n",
    "    - Filter to 2022+ (as discussed with client)\n",
    "    - Remove completely empty columns\n",
    "    - Basic info about the dataset\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path).replace('.xlsx', '')\n",
    "    print(f\"\\nğŸ”§ Cleaning: {filename}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Standardize date column names\n",
    "    date_mapping = {\n",
    "        'datum': 'date',\n",
    "        'dag': 'date'\n",
    "    }\n",
    "    \n",
    "    for old_col, new_col in date_mapping.items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.rename(columns={old_col: new_col})\n",
    "            print(f\"  âœ… Renamed '{old_col}' â†’ '{new_col}'\")\n",
    "    \n",
    "    # FIXED: Convert date column to datetime with proper format handling\n",
    "    if 'date' in df.columns:\n",
    "        original_date_range = f\"{df['date'].min()} to {df['date'].max()}\"\n",
    "        print(f\"  ğŸ“… Original date range: {original_date_range}\")\n",
    "        \n",
    "        # CRITICAL FIX: Only search data needs DD-MM-YYYY parsing\n",
    "        if filename == 'search' and isinstance(df['date'].iloc[0], str):\n",
    "            print(f\"  ğŸš¨ SEARCH DATA: Applying DD-MM-YYYY parsing\")\n",
    "            # Parse as DD-MM-YYYY format (search data specific)\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
    "        else:\n",
    "            print(f\"  âœ… Standard datetime parsing\")\n",
    "            # Standard datetime parsing for all other datasets\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Filter to 2022+ (as discussed with client)\n",
    "        df = df[df['date'].dt.year >= 2022]\n",
    "        filtered_date_range = f\"{df['date'].min()} to {df['date'].max()}\"\n",
    "        print(f\"  ğŸ“… Filtered to 2022+: {filtered_date_range}\")\n",
    "        print(f\"  ğŸ“Š Records after 2022 filter: {len(df)}\")\n",
    "        \n",
    "        # Check for invalid dates\n",
    "        invalid_dates = df['date'].isna().sum()\n",
    "        if invalid_dates > 0:\n",
    "            print(f\"  âš ï¸  {invalid_dates} invalid dates found\")\n",
    "    \n",
    "    # Remove completely empty columns\n",
    "    empty_cols = df.columns[df.isna().all()].tolist()\n",
    "    if empty_cols:\n",
    "        df = df.drop(columns=empty_cols)\n",
    "        print(f\"  ğŸ—‘ï¸  Removed empty columns: {empty_cols}\")\n",
    "    \n",
    "    # Remove unnamed columns (Excel artifacts)\n",
    "    unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "    if unnamed_cols:\n",
    "        df = df.drop(columns=unnamed_cols)\n",
    "        print(f\"  ğŸ—‘ï¸  Removed unnamed columns: {unnamed_cols}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"  ğŸ“Š Final shape: {df.shape}\")\n",
    "    print(f\"  ğŸ“Š Missing values: {df.isna().sum().sum()}\")\n",
    "    print(f\"  ğŸ“Š Duplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    return df, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7acafd84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 3: Special promo processing function\n",
    "def process_promo_data(df):\n",
    "    \"\"\"\n",
    "    Special processing for promo data:\n",
    "    - Pivot promo types into columns\n",
    "    - Create promotion classification (Buy One Get One=1, Limited Time Offer=2, Price Discount=3, No Promotion=0)\n",
    "    - Return processed promo dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¯ Special Promo Processing\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check if this is promo data\n",
    "    if 'promotion_type' not in df.columns:\n",
    "        print(f\"  â„¹ï¸  Not promo data - skipping special processing\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"  ğŸ“Š Original promo shape: {df.shape}\")\n",
    "    print(f\"  ğŸ“Š Promotion types: {df['promotion_type'].unique()}\")\n",
    "    \n",
    "    # Create separate flags by pivoting\n",
    "    promo_counts = df.pivot_table(\n",
    "        index=\"date\", \n",
    "        columns=\"promotion_type\", \n",
    "        aggfunc=\"size\", \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Clean column names\n",
    "    promo_counts.columns.name = None\n",
    "    \n",
    "    # Rename columns to be more code-friendly\n",
    "    column_mapping = {\n",
    "        \"Buy One Get One\": \"buy_one_get_one\",\n",
    "        \"Limited Time Offer\": \"limited_time_offer\", \n",
    "        \"Price Discount\": \"price_discount\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in promo_counts.columns:\n",
    "            promo_counts = promo_counts.rename(columns={old_name: new_name})\n",
    "    \n",
    "    print(f\"  ğŸ“Š Pivoted columns: {list(promo_counts.columns)}\")\n",
    "    \n",
    "    # Assign a single promotion_type value based on priority\n",
    "    def classify_promo(row):\n",
    "        \"\"\"\n",
    "        Classification priority:\n",
    "        1 = Buy One Get One (highest priority)\n",
    "        2 = Limited Time Offer\n",
    "        3 = Price Discount\n",
    "        0 = No Promotion\n",
    "        \"\"\"\n",
    "        # Only check numeric columns (exclude 'date')\n",
    "        numeric_cols = [col for col in row.index if col != 'date']\n",
    "        \n",
    "        if row.get(\"buy_one_get_one\", 0) > 0:\n",
    "            return 1\n",
    "        elif row.get(\"limited_time_offer\", 0) > 0:\n",
    "            return 2\n",
    "        elif row.get(\"price_discount\", 0) > 0:\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    promo_counts[\"promotion_type\"] = promo_counts.apply(classify_promo, axis=1)\n",
    "    \n",
    "    # Keep only date and final promotion_type\n",
    "    promo_final = promo_counts[[\"date\", \"promotion_type\"]].copy()\n",
    "    \n",
    "    print(f\"  ğŸ“Š Final promo shape: {promo_final.shape}\")\n",
    "    print(f\"  ğŸ“Š Promotion distribution:\")\n",
    "    promo_dist = promo_final['promotion_type'].value_counts().sort_index()\n",
    "    for promo_code, count in promo_dist.items():\n",
    "        promo_names = {0: \"No Promotion\", 1: \"Buy One Get One\", 2: \"Limited Time Offer\", 3: \"Price Discount\"}\n",
    "        print(f\"    {promo_code} ({promo_names.get(promo_code, 'Unknown')}): {count}\")\n",
    "    \n",
    "    return promo_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04afc6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Cleaning: email\n",
      "------------------------------\n",
      "Original shape: (104, 2)\n",
      "Columns: ['date', 'email_campaigns']\n",
      "  ğŸ“… Original date range: 2022-01-03 00:00:00 to 2023-12-25 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2023-12-25 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 104\n",
      "  ğŸ“Š Final shape: (104, 2)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/email_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: sales\n",
      "------------------------------\n",
      "Original shape: (260, 2)\n",
      "Columns: ['date', 'sales']\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 2)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/sales_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: tv_promo\n",
      "------------------------------\n",
      "Original shape: (260, 3)\n",
      "Columns: ['datum', 'tv_promo_grps', 'tv_promo_cost']\n",
      "  âœ… Renamed 'datum' â†’ 'date'\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/tv_promo_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: tv_branding\n",
      "------------------------------\n",
      "Original shape: (260, 7)\n",
      "Columns: ['datum', 'tv_branding_grps', 'tv_branding_cost', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6']\n",
      "  âœ… Renamed 'datum' â†’ 'date'\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ—‘ï¸  Removed empty columns: ['Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6']\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/tv_branding_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: radio_local\n",
      "------------------------------\n",
      "Original shape: (260, 3)\n",
      "Columns: ['dag', 'radio_local_grps', 'radio_local_cost']\n",
      "  âœ… Renamed 'dag' â†’ 'date'\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/radio_local_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: social\n",
      "------------------------------\n",
      "Original shape: (260, 3)\n",
      "Columns: ['date', 'impressions', 'costs']\n",
      "  ğŸ“… Original date range: 2020-01-06 to 2024-12-23\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/social_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: promo\n",
      "------------------------------\n",
      "Original shape: (81, 6)\n",
      "Columns: ['date', 'promotion_type', 'year', 'month', 'quarter', 'week_in_quarter']\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-11-04 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-10 00:00:00 to 2024-11-04 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 47\n",
      "  ğŸ“Š Final shape: (47, 6)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "\n",
      "ğŸ¯ Special Promo Processing\n",
      "------------------------------\n",
      "  ğŸ“Š Original promo shape: (47, 6)\n",
      "  ğŸ“Š Promotion types: ['Price Discount' 'Limited Time Offer' 'Buy One Get One']\n",
      "  ğŸ“Š Pivoted columns: ['date', 'buy_one_get_one', 'limited_time_offer', 'price_discount']\n",
      "  ğŸ“Š Final promo shape: (47, 2)\n",
      "  ğŸ“Š Promotion distribution:\n",
      "    1 (Buy One Get One): 15\n",
      "    2 (Limited Time Offer): 13\n",
      "    3 (Price Discount): 19\n",
      "  ğŸ’¾ Saved: ../data/interim/promo_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: radio_national\n",
      "------------------------------\n",
      "Original shape: (260, 3)\n",
      "Columns: ['dag', 'radio_national_grps', 'radio_national_cost']\n",
      "  âœ… Renamed 'dag' â†’ 'date'\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/radio_national_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: search\n",
      "------------------------------\n",
      "Original shape: (260, 3)\n",
      "Columns: ['date', 'impressions', 'cost']\n",
      "  ğŸ“… Original date range: 01-01-2024 to 31-10-2022\n",
      "  ğŸš¨ SEARCH DATA: Applying DD-MM-YYYY parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 3)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/search_basic_clean.csv\n",
      "\n",
      "ğŸ”§ Cleaning: ooh\n",
      "------------------------------\n",
      "Original shape: (260, 2)\n",
      "Columns: ['date', 'ooh_spend']\n",
      "  ğŸ“… Original date range: 2020-01-06 00:00:00 to 2024-12-23 00:00:00\n",
      "  âœ… Standard datetime parsing\n",
      "  ğŸ“… Filtered to 2022+: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "  ğŸ“Š Records after 2022 filter: 156\n",
      "  ğŸ“Š Final shape: (156, 2)\n",
      "  ğŸ“Š Missing values: 0\n",
      "  ğŸ“Š Duplicate rows: 0\n",
      "  ğŸ’¾ Saved: ../data/interim/ooh_basic_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Clean each dataset and save\n",
    "cleaned_dir = '../data/interim'\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "cleaned_datasets = {}\n",
    "\n",
    "for file_path in excel_files:\n",
    "    try:\n",
    "        df_clean, name = basic_clean_dataset(file_path)\n",
    "        \n",
    "        # Special processing for promo data\n",
    "        if name == 'promo':\n",
    "            df_clean = process_promo_data(df_clean)\n",
    "        \n",
    "        # Save cleaned dataset\n",
    "        output_path = os.path.join(cleaned_dir, f\"{name}_basic_clean.csv\")\n",
    "        df_clean.to_csv(output_path, index=False)\n",
    "        \n",
    "        cleaned_datasets[name] = df_clean\n",
    "        print(f\"  ğŸ’¾ Saved: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error cleaning {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257f20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ SUMMARY - Cleaned Datasets (2022+ Only)\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š EMAIL:\n",
      "   Shape: (104, 2)\n",
      "   Columns: ['date', 'email_campaigns']\n",
      "   Date range: 2022-01-03 00:00:00 to 2023-12-25 00:00:00\n",
      "   Total records: 104\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š SALES:\n",
      "   Shape: (156, 2)\n",
      "   Columns: ['date', 'sales']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š TV_PROMO:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'tv_promo_grps', 'tv_promo_cost']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š TV_BRANDING:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'tv_branding_grps', 'tv_branding_cost']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š RADIO_LOCAL:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'radio_local_grps', 'radio_local_cost']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š SOCIAL:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'impressions', 'costs']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š PROMO:\n",
      "   Shape: (47, 2)\n",
      "   Columns: ['date', 'promotion_type']\n",
      "   Date range: 2022-01-10 00:00:00 to 2024-11-04 00:00:00\n",
      "   Total records: 47\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š RADIO_NATIONAL:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'radio_national_grps', 'radio_national_cost']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š SEARCH:\n",
      "   Shape: (156, 3)\n",
      "   Columns: ['date', 'impressions', 'cost']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "ğŸ“Š OOH:\n",
      "   Shape: (156, 2)\n",
      "   Columns: ['date', 'ooh_spend']\n",
      "   Date range: 2022-01-03 00:00:00 to 2024-12-23 00:00:00\n",
      "   Total records: 156\n",
      "   âœ… No missing values\n",
      "\n",
      "âœ… Basic cleaning complete!\n",
      "ğŸ“ Cleaned files saved in: ../data/interim\n",
      "ğŸ“… All datasets filtered to 2022+ (as discussed with client)\n",
      "ğŸ¯ Promo data specially processed with promotion classification\n",
      "ğŸ““ Next: Run 01_data_inspection.py to analyze each dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Summary of all cleaned datasets\n",
    "print(f\"\\nğŸ“‹ SUMMARY - Cleaned Datasets (2022+ Only)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in cleaned_datasets.items():\n",
    "    print(f\"\\nğŸ“Š {name.upper()}:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "        print(f\"   Total records: {len(df.dropna(subset=['date']))}\")\n",
    "    \n",
    "    missing = df.isna().sum().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"   âš ï¸  Missing values: {missing}\")\n",
    "    else:\n",
    "        print(f\"   âœ… No missing values\")\n",
    "\n",
    "print(f\"\\nâœ… Basic cleaning complete!\")\n",
    "print(f\"ğŸ“ Cleaned files saved in: {cleaned_dir}\")\n",
    "print(f\"ğŸ“… All datasets filtered to 2022+ (as discussed with client)\")\n",
    "print(f\"ğŸ¯ Promo data specially processed with promotion classification\")\n",
    "print(f\"ğŸ““ Next: Run 01_data_inspection.py to analyze each dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb62a42",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "**Files created:**\n",
    "- Each dataset saved as `{name}_basic_clean.csv` in `data/interim/`\n",
    "- **All filtered to 2022+** (as discussed with client)\n",
    "- **Promo data specially processed** with promotion type classification\n",
    "- Ready for individual inspection\n",
    "\n",
    "**Promo Processing Details:**\n",
    "- Pivoted promotion types into separate columns\n",
    "- Applied priority-based classification:\n",
    "  - 1 = Buy One Get One (highest priority)\n",
    "  - 2 = Limited Time Offer  \n",
    "  - 3 = Price Discount\n",
    "  - 0 = No Promotion\n",
    "\n",
    "**Next steps:**\n",
    "1. **Inspect each dataset individually** in next script\n",
    "2. **Identify data quality issues** (gaps, outliers, inconsistencies)\n",
    "3. **Make decisions** about how to handle each issue\n",
    "4. **Then** proceed with unification\n",
    "\n",
    "**No unification yet** - we want to understand each dataset first! "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
