{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f38046",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Modeling\n",
    "\n",
    "**Goal**: Prepare cleaned datasets for machine learning modeling\n",
    "\n",
    "**Process:**\n",
    "1. Load cleaned datasets\n",
    "2. Quality verification  \n",
    "3. Minimal outlier treatment\n",
    "4. Time feature engineering\n",
    "5. Save preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß Data Preprocessing for Modeling\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120a157",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load all cleaned datasets\n",
    "interim_dir = '../data/interim'\n",
    "cleaned_files = glob.glob(os.path.join(interim_dir, '*_basic_clean.csv'))\n",
    "\n",
    "datasets = {}\n",
    "for file_path in cleaned_files:\n",
    "    filename = os.path.basename(file_path).replace('_basic_clean.csv', '')\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    datasets[filename] = df\n",
    "    print(f\"  üìÅ {filename}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c6618",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Quick quality check\n",
    "def quick_quality_check(df, name):\n",
    "    print(f\"\\n‚úÖ {name.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "    \n",
    "    missing = df.isnull().sum().sum()\n",
    "    print(f\"  Missing values: {missing}\")\n",
    "    \n",
    "    # Check numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'date' in numeric_cols:\n",
    "        numeric_cols.remove('date')\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if len(df[col].dropna()) > 0:\n",
    "            data = df[col].dropna()\n",
    "            Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n",
    "            outlier_pct = len(outliers) / len(data) * 100\n",
    "            print(f\"  {col}: outliers={outlier_pct:.1f}%\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    quick_quality_check(df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaa209",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Minimal outlier treatment\n",
    "def handle_outliers_minimal(df, name):\n",
    "    print(f\"\\nüéØ Outlier treatment: {name}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'date' in numeric_cols:\n",
    "        numeric_cols.remove('date')\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data = df_clean[col].dropna()\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        \n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_mask = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            outlier_pct = outliers_count / len(data) * 100\n",
    "            \n",
    "            # Conservative treatment: only cap if <2% outliers\n",
    "            if outlier_pct < 2:\n",
    "                df_clean.loc[df_clean[col] < lower_bound, col] = lower_bound\n",
    "                df_clean.loc[df_clean[col] > upper_bound, col] = upper_bound\n",
    "                print(f\"  {col}: {outliers_count} outliers capped ({outlier_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  {col}: {outliers_count} outliers kept ({outlier_pct:.1f}%)\")\n",
    "    \n",
    "    return df_clean, outlier_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130454bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Time feature engineering\n",
    "def engineer_time_features(df, name):\n",
    "    print(f\"\\n‚öôÔ∏è Time features: {name}\")\n",
    "    \n",
    "    if 'date' not in df.columns:\n",
    "        print(\"  No date column - skipping\")\n",
    "        return df\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df_features['year'] = df_features['date'].dt.year\n",
    "    df_features['month'] = df_features['date'].dt.month\n",
    "    df_features['week'] = df_features['date'].dt.isocalendar().week\n",
    "    df_features['quarter'] = df_features['date'].dt.quarter\n",
    "    \n",
    "    # Cyclical features for seasonality\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['week_sin'] = np.sin(2 * np.pi * df_features['week'] / 52)\n",
    "    df_features['week_cos'] = np.cos(2 * np.pi * df_features['week'] / 52)\n",
    "    \n",
    "    # Business features\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]: return 'winter'\n",
    "        elif month in [3, 4, 5]: return 'spring'\n",
    "        elif month in [6, 7, 8]: return 'summer'\n",
    "        else: return 'autumn'\n",
    "    \n",
    "    df_features['season'] = df_features['month'].apply(get_season)\n",
    "    \n",
    "    # Holiday periods\n",
    "    def is_holiday_period(date):\n",
    "        month, day = date.month, date.day\n",
    "        if (month == 1 and day <= 7) or (month == 12 and day >= 25): return 1\n",
    "        elif month in [7, 8]: return 1\n",
    "        elif month in [3, 4] and 15 <= day <= 25: return 1\n",
    "        else: return 0\n",
    "    \n",
    "    df_features['holiday_period'] = df_features['date'].apply(is_holiday_period)\n",
    "    df_features['is_month_end'] = (df_features['date'].dt.day >= 25).astype(int)\n",
    "    \n",
    "    new_features = df_features.shape[1] - df.shape[1]\n",
    "    print(f\"  Added {new_features} time features\")\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8bf9b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Process all datasets\n",
    "print(f\"\\nüîß Processing all datasets...\")\n",
    "\n",
    "preprocessed_datasets = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*10} {name.upper()} {'='*10}\")\n",
    "    \n",
    "    # Outlier treatment\n",
    "    df_step1, _ = handle_outliers_minimal(df, name)\n",
    "    \n",
    "    # Time features\n",
    "    df_final = engineer_time_features(df_step1, name)\n",
    "    \n",
    "    preprocessed_datasets[name] = df_final\n",
    "    print(f\"  Final shape: {df.shape} ‚Üí {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e48310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison for key datasets\n",
    "def plot_key_distributions(original_datasets, preprocessed_datasets):\n",
    "    # Only plot distributions for datasets with meaningful numeric variables\n",
    "    key_datasets = ['email', 'facebook', 'google', 'ooh']  # Most important media channels\n",
    "    \n",
    "    for name in key_datasets:\n",
    "        if name not in original_datasets:\n",
    "            continue\n",
    "            \n",
    "        df_orig = original_datasets[name]\n",
    "        df_proc = preprocessed_datasets[name]\n",
    "        \n",
    "        # Get main numeric column (usually the spend/impression/click column)\n",
    "        numeric_cols = df_orig.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'date' in numeric_cols:\n",
    "            numeric_cols.remove('date')\n",
    "        \n",
    "        if not numeric_cols:\n",
    "            continue\n",
    "            \n",
    "        # Plot the main variable\n",
    "        main_col = numeric_cols[0]  # Usually the most important metric\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Original\n",
    "        data_orig = df_orig[main_col].dropna()\n",
    "        axes[0].hist(data_orig, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title(f'{name.title()} - {main_col} (Original)')\n",
    "        skew_orig = stats.skew(data_orig)\n",
    "        axes[0].text(0.02, 0.98, f'Skew: {skew_orig:.2f}', \n",
    "                    transform=axes[0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Processed\n",
    "        data_proc = df_proc[main_col].dropna()\n",
    "        axes[1].hist(data_proc, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[1].set_title(f'{name.title()} - {main_col} (Processed)')\n",
    "        skew_proc = stats.skew(data_proc)\n",
    "        axes[1].text(0.02, 0.98, f'Skew: {skew_proc:.2f}', \n",
    "                    transform=axes[1].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_key_distributions(datasets, preprocessed_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce19ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüíæ Saving preprocessed datasets...\")\n",
    "\n",
    "for name, df in preprocessed_datasets.items():\n",
    "    output_path = os.path.join(processed_dir, f\"{name}_preprocessed.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"  ‚úÖ {name}_preprocessed.csv ({df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\nüìã PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nDatasets processed: {len(preprocessed_datasets)}\")\n",
    "for name, df in preprocessed_datasets.items():\n",
    "    original_shape = datasets[name].shape\n",
    "    final_shape = df.shape\n",
    "    features_added = final_shape[1] - original_shape[1]\n",
    "    print(f\"  {name}: {original_shape} ‚Üí {final_shape} (+{features_added} features)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for next steps:\")\n",
    "print(f\"  1. Data unification\")\n",
    "print(f\"  2. EDA on unified dataset\")\n",
    "print(f\"  3. Media Mix Modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd0feb",
   "metadata": {},
   "source": [
    "## Preprocessing Summary\n",
    "\n",
    "**What was done:**\n",
    "- Quality verification (no missing values, minimal outliers)\n",
    "- Conservative outlier treatment (only extreme cases)\n",
    "- Time feature engineering (cyclical features for seasonality)\n",
    "- Business features (seasons, holidays, month-end effects)\n",
    "\n",
    "**Output:**\n",
    "- Individual preprocessed datasets saved in `data/processed/`\n",
    "- Ready for unification and modeling\n",
    "\n",
    "**Next:** Run data unification to combine all datasets "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
