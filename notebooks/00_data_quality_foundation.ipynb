{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 00 - Data Quality Foundation\n",
        "\n",
        "**Establishing Clean Dataset for MMM Analysis**\n",
        "\n",
        "**Research Team**: Data Science MMM Development  \n",
        "**Project**: Ice Cream Company Media Mix Modeling  \n",
        "**Objective**: Clean and validate raw data to ensure reliable MMM modeling foundation\n",
        "\n",
        "## \ud83d\udd2c Research Methodology\n",
        "\n",
        "**Key Mathematical Formulations:**\n",
        "- Missing Value Rate = (Missing Values / Total Observations) \u00d7 100%\n",
        "- Data Quality Score = (Complete Records / Total Records) \u00d7 100%\n",
        "\n",
        "**Quality Standards:**\n",
        "- Temporal validation (no data leakage)\n",
        "- Statistical significance testing\n",
        "- Business logic validation\n",
        "- Reproducible analysis\n",
        "\n",
        "**Report Documentation**: All analyses documented for stakeholder reporting\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Data Cleaning\n",
        "\n",
        "**Goal**: Clean individual datasets and save them separately for inspection.\n",
        "\n",
        "**What we do:**\n",
        "1. Load raw Excel files\n",
        "2. Basic cleaning (dates, remove empty columns, handle missing values)\n",
        "3. **Filter to 2022+ only** (as discussed with client)\n",
        "4. **FIXED: Proper date parsing** for search data (DD-MM-YYYY string format)\n",
        "5. Special processing for promo data (pivot and classification)\n",
        "6. Save each dataset as clean CSV\n",
        "7. **NO UNIFICATION YET** - we inspect each dataset first\n",
        "\n",
        "**What we DON'T do:**\n",
        "- Merge datasets\n",
        "- Complex transformations beyond promo processing\n",
        "- Weekly aggregation (yet)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\ud83e\uddf9 Basic Data Cleaning - Individual Datasets\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\ud83d\udea8 FIXED: Search data date parsing (DD-MM-YYYY string format)\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load all raw Excel files\n",
        "raw_dir = '../data/raw'\n",
        "excel_files = glob.glob(os.path.join(raw_dir, '*.xlsx'))\n",
        "\n",
        "print(f\"Found {len(excel_files)} Excel files:\")\n",
        "for file in excel_files:\n",
        "    print(f\"  \ud83d\udcc1 {os.path.basename(file)}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: FIXED Basic cleaning function with proper date handling\n",
        "def basic_clean_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Basic cleaning for individual dataset:\n",
        "    - Load Excel file\n",
        "    - Standardize date column names\n",
        "    - FIXED: Convert dates to datetime (special handling for search data)\n",
        "    - Filter to 2022+ (as discussed with client)\n",
        "    - Remove completely empty columns\n",
        "    - Basic info about the dataset\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(file_path).replace('.xlsx', '')\n",
        "    print(f\"\\n\ud83d\udd27 Cleaning: {filename}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Load data\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Standardize date column names\n",
        "    date_mapping = {\n",
        "        'datum': 'date',\n",
        "        'dag': 'date'\n",
        "    }\n",
        "    \n",
        "    for old_col, new_col in date_mapping.items():\n",
        "        if old_col in df.columns:\n",
        "            df = df.rename(columns={old_col: new_col})\n",
        "            print(f\"  \u2705 Renamed '{old_col}' \u2192 '{new_col}'\")\n",
        "    \n",
        "    # FIXED: Convert date column to datetime with proper format handling\n",
        "    if 'date' in df.columns:\n",
        "        original_date_range = f\"{df['date'].min()} to {df['date'].max()}\"\n",
        "        print(f\"  \ud83d\udcc5 Original date range: {original_date_range}\")\n",
        "        \n",
        "        # CRITICAL FIX: Only search data needs DD-MM-YYYY parsing\n",
        "        if filename == 'search' and isinstance(df['date'].iloc[0], str):\n",
        "            print(f\"  \ud83d\udea8 SEARCH DATA: Applying DD-MM-YYYY parsing\")\n",
        "            # Parse as DD-MM-YYYY format (search data specific)\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
        "        else:\n",
        "            print(f\"  \u2705 Standard datetime parsing\")\n",
        "            # Standard datetime parsing for all other datasets\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        \n",
        "        # Filter to 2022+ (as discussed with client)\n",
        "        df = df[df['date'].dt.year >= 2022]\n",
        "        filtered_date_range = f\"{df['date'].min()} to {df['date'].max()}\"\n",
        "        print(f\"  \ud83d\udcc5 Filtered to 2022+: {filtered_date_range}\")\n",
        "        print(f\"  \ud83d\udcca Records after 2022 filter: {len(df)}\")\n",
        "        \n",
        "        # Check for invalid dates\n",
        "        invalid_dates = df['date'].isna().sum()\n",
        "        if invalid_dates > 0:\n",
        "            print(f\"  \u26a0\ufe0f  {invalid_dates} invalid dates found\")\n",
        "    \n",
        "    # Remove completely empty columns\n",
        "    empty_cols = df.columns[df.isna().all()].tolist()\n",
        "    if empty_cols:\n",
        "        df = df.drop(columns=empty_cols)\n",
        "        print(f\"  \ud83d\uddd1\ufe0f  Removed empty columns: {empty_cols}\")\n",
        "    \n",
        "    # Remove unnamed columns (Excel artifacts)\n",
        "    unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
        "    if unnamed_cols:\n",
        "        df = df.drop(columns=unnamed_cols)\n",
        "        print(f\"  \ud83d\uddd1\ufe0f  Removed unnamed columns: {unnamed_cols}\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"  \ud83d\udcca Final shape: {df.shape}\")\n",
        "    print(f\"  \ud83d\udcca Missing values: {df.isna().sum().sum()}\")\n",
        "    print(f\"  \ud83d\udcca Duplicate rows: {df.duplicated().sum()}\")\n",
        "    \n",
        "    return df, filename\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Special promo processing function\n",
        "def process_promo_data(df):\n",
        "    \"\"\"\n",
        "    Special processing for promo data:\n",
        "    - Pivot promo types into columns\n",
        "    - Create promotion classification (Buy One Get One=1, Limited Time Offer=2, Price Discount=3, No Promotion=0)\n",
        "    - Return processed promo dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83c\udfaf Special Promo Processing\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Check if this is promo data\n",
        "    if 'promotion_type' not in df.columns:\n",
        "        print(f\"  \u2139\ufe0f  Not promo data - skipping special processing\")\n",
        "        return df\n",
        "    \n",
        "    print(f\"  \ud83d\udcca Original promo shape: {df.shape}\")\n",
        "    print(f\"  \ud83d\udcca Promotion types: {df['promotion_type'].unique()}\")\n",
        "    \n",
        "    # Create separate flags by pivoting\n",
        "    promo_counts = df.pivot_table(\n",
        "        index=\"date\", \n",
        "        columns=\"promotion_type\", \n",
        "        aggfunc=\"size\", \n",
        "        fill_value=0\n",
        "    ).reset_index()\n",
        "    \n",
        "    # Clean column names\n",
        "    promo_counts.columns.name = None\n",
        "    \n",
        "    # Rename columns to be more code-friendly\n",
        "    column_mapping = {\n",
        "        \"Buy One Get One\": \"buy_one_get_one\",\n",
        "        \"Limited Time Offer\": \"limited_time_offer\", \n",
        "        \"Price Discount\": \"price_discount\"\n",
        "    }\n",
        "    \n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in promo_counts.columns:\n",
        "            promo_counts = promo_counts.rename(columns={old_name: new_name})\n",
        "    \n",
        "    print(f\"  \ud83d\udcca Pivoted columns: {list(promo_counts.columns)}\")\n",
        "    \n",
        "    # Assign a single promotion_type value based on priority\n",
        "    def classify_promo(row):\n",
        "        \"\"\"\n",
        "        Classification priority:\n",
        "        1 = Buy One Get One (highest priority)\n",
        "        2 = Limited Time Offer\n",
        "        3 = Price Discount\n",
        "        0 = No Promotion\n",
        "        \"\"\"\n",
        "        # Only check numeric columns (exclude 'date')\n",
        "        numeric_cols = [col for col in row.index if col != 'date']\n",
        "        \n",
        "        if row.get(\"buy_one_get_one\", 0) > 0:\n",
        "            return 1\n",
        "        elif row.get(\"limited_time_offer\", 0) > 0:\n",
        "            return 2\n",
        "        elif row.get(\"price_discount\", 0) > 0:\n",
        "            return 3\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    promo_counts[\"promotion_type\"] = promo_counts.apply(classify_promo, axis=1)\n",
        "    \n",
        "    # Keep only date and final promotion_type\n",
        "    promo_final = promo_counts[[\"date\", \"promotion_type\"]].copy()\n",
        "    \n",
        "    print(f\"  \ud83d\udcca Final promo shape: {promo_final.shape}\")\n",
        "    print(f\"  \ud83d\udcca Promotion distribution:\")\n",
        "    promo_dist = promo_final['promotion_type'].value_counts().sort_index()\n",
        "    for promo_code, count in promo_dist.items():\n",
        "        promo_names = {0: \"No Promotion\", 1: \"Buy One Get One\", 2: \"Limited Time Offer\", 3: \"Price Discount\"}\n",
        "        print(f\"    {promo_code} ({promo_names.get(promo_code, 'Unknown')}): {count}\")\n",
        "    \n",
        "    return promo_final\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Clean each dataset and save\n",
        "cleaned_dir = '../data/interim'\n",
        "os.makedirs(cleaned_dir, exist_ok=True)\n",
        "\n",
        "cleaned_datasets = {}\n",
        "\n",
        "for file_path in excel_files:\n",
        "    try:\n",
        "        df_clean, name = basic_clean_dataset(file_path)\n",
        "        \n",
        "        # Special processing for promo data\n",
        "        if name == 'promo':\n",
        "            df_clean = process_promo_data(df_clean)\n",
        "        \n",
        "        # Save cleaned dataset\n",
        "        output_path = os.path.join(cleaned_dir, f\"{name}_basic_clean.csv\")\n",
        "        df_clean.to_csv(output_path, index=False)\n",
        "        \n",
        "        cleaned_datasets[name] = df_clean\n",
        "        print(f\"  \ud83d\udcbe Saved: {output_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  \u274c Error cleaning {file_path}: {e}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Summary of all cleaned datasets\n",
        "print(f\"\\n\ud83d\udccb SUMMARY - Cleaned Datasets (2022+ Only)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, df in cleaned_datasets.items():\n",
        "    print(f\"\\n\ud83d\udcca {name.upper()}:\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Columns: {list(df.columns)}\")\n",
        "    \n",
        "    if 'date' in df.columns:\n",
        "        print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        print(f\"   Total records: {len(df.dropna(subset=['date']))}\")\n",
        "    \n",
        "    missing = df.isna().sum().sum()\n",
        "    if missing > 0:\n",
        "        print(f\"   \u26a0\ufe0f  Missing values: {missing}\")\n",
        "    else:\n",
        "        print(f\"   \u2705 No missing values\")\n",
        "\n",
        "print(f\"\\n\u2705 Basic cleaning complete!\")\n",
        "print(f\"\ud83d\udcc1 Cleaned files saved in: {cleaned_dir}\")\n",
        "print(f\"\ud83d\udcc5 All datasets filtered to 2022+ (as discussed with client)\")\n",
        "print(f\"\ud83c\udfaf Promo data specially processed with promotion classification\")\n",
        "print(f\"\ud83d\udcd3 Next: Run 01_data_inspection.py to analyze each dataset\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's Next?\n",
        "\n",
        "**Files created:**\n",
        "- Each dataset saved as `{name}_basic_clean.csv` in `data/interim/`\n",
        "- **All filtered to 2022+** (as discussed with client)\n",
        "- **Promo data specially processed** with promotion type classification\n",
        "- Ready for individual inspection\n",
        "\n",
        "**Promo Processing Details:**\n",
        "- Pivoted promotion types into separate columns\n",
        "- Applied priority-based classification:\n",
        "  - 1 = Buy One Get One (highest priority)\n",
        "  - 2 = Limited Time Offer  \n",
        "  - 3 = Price Discount\n",
        "  - 0 = No Promotion\n",
        "\n",
        "**Next steps:**\n",
        "1. **Inspect each dataset individually** in next script\n",
        "2. **Identify data quality issues** (gaps, outliers, inconsistencies)\n",
        "3. **Make decisions** about how to handle each issue\n",
        "4. **Then** proceed with unification\n",
        "\n",
        "**No unification yet** - we want to understand each dataset first! \n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}