{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 03 - EDA-Informed Model Development\n",
        "\n",
        "**Feature Selection Based on Exploratory Analysis**\n",
        "\n",
        "**Research Team**: Data Science MMM Development  \n",
        "**Project**: Ice Cream Company Media Mix Modeling  \n",
        "**Objective**: Select optimal features based on EDA insights for MMM modeling\n",
        "\n",
        "## \ud83d\udd2c Research Methodology\n",
        "\n",
        "**Key Mathematical Formulations:**\n",
        "- Feature Importance: |coefficient| / std(feature)\n",
        "- VIF (Multicollinearity): 1 / (1 - R\u00b2\u1d62)\n",
        "- Selection Criteria: Business Logic + Statistical Significance\n",
        "\n",
        "**Quality Standards:**\n",
        "- Temporal validation (no data leakage)\n",
        "- Statistical significance testing\n",
        "- Business logic validation\n",
        "- Reproducible analysis\n",
        "\n",
        "**Report Documentation**: All analyses documented for stakeholder reporting\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 - EDA-Informed Feature Optimization for MMM\n",
        "\n",
        "**Goal**: Optimize features based on EDA insights for Media Mix Modeling\n",
        "\n",
        "**EDA Key Findings:**\n",
        "- Weather variables are CRITICAL (sunshine: 0.664, temp: 0.626 correlation)\n",
        "- Seasonality is crucial (week_cos: 0.724 correlation - strongest predictor!)\n",
        "- All media channels show effectiveness and budget justification\n",
        "- Perfect 3-year data quality with 100% media channel coverage\n",
        "\n",
        "**Process:**\n",
        "1. Load both corrected datasets (with EDA insights)\n",
        "2. Business-informed feature selection (keep weather + seasonality!)\n",
        "3. Remove only true redundancies and multicollinearity\n",
        "4. Preserve business-critical relationships\n",
        "5. Create final MMM-ready datasets\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\ud83c\udfaf EDA-INFORMED FEATURE OPTIMIZATION FOR MMM\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\ud83d\udd0d Based on EDA insights: Weather & seasonality are CRITICAL for ice cream!\")\n",
        "print(\"\ud83d\udcca Key correlations: week_cos(0.724), sunshine(0.664), temperature(0.626)\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load both corrected unified datasets\n",
        "print(\"\ud83d\udcca Loading corrected unified datasets...\")\n",
        "\n",
        "# Dataset 1: Complete channels including email (2022-2023)\n",
        "complete_path = 'data/processed/mmm_dataset_complete_channels_2022_2023.csv'\n",
        "# Dataset 2: Consistent channels excluding email (2022-2024)\n",
        "consistent_path = 'data/processed/mmm_dataset_consistent_channels_2022_2024.csv'\n",
        "\n",
        "try:\n",
        "    df_complete = pd.read_csv(complete_path)\n",
        "    df_complete['date'] = pd.to_datetime(df_complete['date'])\n",
        "    print(f\"\u2705 Complete channels (2022-2023): {df_complete.shape}\")\n",
        "    print(f\"  \ud83d\udcc5 Date range: {df_complete['date'].min().date()} to {df_complete['date'].max().date()}\")\n",
        "    print(f\"  \ud83d\udce7 Includes email: {'email_email_campaigns' in df_complete.columns}\")\n",
        "    \n",
        "    df_consistent = pd.read_csv(consistent_path)\n",
        "    df_consistent['date'] = pd.to_datetime(df_consistent['date'])\n",
        "    print(f\"\u2705 Consistent channels (2022-2024): {df_consistent.shape}\")\n",
        "    print(f\"  \ud83d\udcc5 Date range: {df_consistent['date'].min().date()} to {df_consistent['date'].max().date()}\")\n",
        "    print(f\"  \ud83d\udce7 Excludes email: {'email_email_campaigns' not in df_consistent.columns}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error loading datasets: {e}\")\n",
        "    exit()\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Analyze cross-channel correlations (the main issue!)\n",
        "def analyze_cross_channel_correlations(df, threshold=0.75):\n",
        "    \"\"\"\n",
        "    Analyze correlations between GRPs and Spend across all channels\n",
        "    This is the main multicollinearity issue in MMM!\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udd0d CROSS-CHANNEL CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 45)\n",
        "    \n",
        "    # Get all numeric columns except date and basic features\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    exclude_cols = ['date', 'sales', 'year', 'month', 'dayofyear', 'week', 'quarter', \n",
        "                   'month_sin', 'month_cos', 'week_sin', 'week_cos', 'holiday_period']\n",
        "    \n",
        "    media_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "    \n",
        "    print(f\"\ud83d\udcfa Media/Weather columns to analyze: {len(media_cols)}\")\n",
        "    \n",
        "    # Calculate correlation matrix for media variables\n",
        "    if len(media_cols) < 2:\n",
        "        print(\"Not enough media columns for correlation analysis\")\n",
        "        return [], []\n",
        "    \n",
        "    corr_matrix = df[media_cols].corr().abs()\n",
        "    \n",
        "    # Find high correlations\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_value = corr_matrix.iloc[i, j]\n",
        "            if corr_value > threshold:\n",
        "                pair = (corr_matrix.columns[i], corr_matrix.columns[j], corr_value)\n",
        "                high_corr_pairs.append(pair)\n",
        "    \n",
        "    # Sort by correlation strength\n",
        "    high_corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    print(f\"\\n\u26a0\ufe0f  HIGH CORRELATIONS FOUND (>{threshold}):\")\n",
        "    if high_corr_pairs:\n",
        "        for col1, col2, corr in high_corr_pairs:\n",
        "            print(f\"  {col1} \u2194 {col2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(f\"  \u2705 No high correlations found\")\n",
        "    \n",
        "    # Identify GRP vs Spend pairs specifically\n",
        "    grp_spend_pairs = []\n",
        "    for col1, col2, corr in high_corr_pairs:\n",
        "        if ('grp' in col1.lower() and ('spend' in col2.lower() or 'cost' in col2.lower())) or \\\n",
        "           ('grp' in col2.lower() and ('spend' in col1.lower() or 'cost' in col1.lower())):\n",
        "            grp_spend_pairs.append((col1, col2, corr))\n",
        "    \n",
        "    if grp_spend_pairs:\n",
        "        print(f\"\\n\ud83d\udea8 GRP vs SPEND PERFECT CORRELATIONS (MMM Problem!):\")\n",
        "        for col1, col2, corr in grp_spend_pairs:\n",
        "            print(f\"  {col1} \u2194 {col2}: {corr:.3f}\")\n",
        "    \n",
        "    return high_corr_pairs, grp_spend_pairs\n",
        "\n",
        "# Analyze correlations for both datasets\n",
        "print(f\"\\n\ud83d\udd0d ANALYZING BOTH DATASETS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca COMPLETE CHANNELS DATASET (2022-2023):\")\n",
        "high_corrs_complete, grp_spend_corrs_complete = analyze_cross_channel_correlations(df_complete)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca CONSISTENT CHANNELS DATASET (2022-2024):\")\n",
        "high_corrs_consistent, grp_spend_corrs_consistent = analyze_cross_channel_correlations(df_consistent)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: EDA-Informed Feature optimization for MMM\n",
        "def optimize_features_for_mmm(df, grp_spend_corrs, high_corrs, dataset_name=\"\"):\n",
        "    \"\"\"\n",
        "    Optimize features for MMM based on EDA insights and business logic\n",
        "    \n",
        "    EDA KEY INSIGHTS:\n",
        "    - Weather variables CRITICAL for ice cream (sunshine: 0.664, temp: 0.626)\n",
        "    - Seasonality crucial (week_cos: 0.724 - strongest predictor!)\n",
        "    - All media channels effective and budget-justified\n",
        "    \"\"\"\n",
        "    print(f\"\\n\u2699\ufe0f EDA-INFORMED FEATURE OPTIMIZATION - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\ud83d\udd0d Applying business insights from EDA analysis...\")\n",
        "    \n",
        "    df_optimized = df.copy()\n",
        "    removed_features = []\n",
        "    \n",
        "    print(f\"\ud83d\udd27 Starting with {df.shape[1]} features\")\n",
        "    \n",
        "    # 1. Remove redundant time features (EDA-informed: keep crucial seasonality!)\n",
        "    print(f\"\\n1\ufe0f\u20e3 EDA-Informed Time Feature Optimization...\")\n",
        "    print(f\"    \ud83d\udd0d EDA insight: week_cos has 0.724 correlation - KEEP!\")\n",
        "    print(f\"    \ud83d\udd0d EDA insight: month_sin/cos capture ice cream seasonality - KEEP!\")\n",
        "    \n",
        "    # Remove only truly redundant linear time features\n",
        "    redundant_time = ['year', 'month', 'week', 'quarter', 'dayofyear']\n",
        "    for feature in redundant_time:\n",
        "        if feature in df_optimized.columns:\n",
        "            df_optimized = df_optimized.drop(columns=[feature])\n",
        "            removed_features.append(f\"{feature} (linear time - redundant with cyclical)\")\n",
        "            print(f\"  \u274c Removed: {feature} (keeping cyclical versions)\")\n",
        "    \n",
        "    # KEEP the important cyclical features based on EDA\n",
        "    cyclical_features = ['month_sin', 'month_cos', 'week_sin', 'week_cos']\n",
        "    for feature in cyclical_features:\n",
        "        if feature in df_optimized.columns:\n",
        "            if feature == 'week_cos':\n",
        "                print(f\"  \u2705 KEPT: {feature} (0.724 correlation - strongest predictor!)\")\n",
        "            else:\n",
        "                print(f\"  \u2705 KEPT: {feature} (crucial for ice cream seasonality)\")\n",
        "    \n",
        "    # 2. Handle GRP vs Spend multicollinearity (keep spend, remove GRPs)\n",
        "    print(f\"\\n2\ufe0f\u20e3 Handling GRP vs Spend multicollinearity...\")\n",
        "    for col1, col2, corr in grp_spend_corrs:\n",
        "        # Determine which one to keep (prefer spend/cost)\n",
        "        if 'grp' in col1.lower() and ('spend' in col2.lower() or 'cost' in col2.lower()):\n",
        "            # Keep col2 (spend), remove col1 (GRP)\n",
        "            if col1 in df_optimized.columns:\n",
        "                df_optimized = df_optimized.drop(columns=[col1])\n",
        "                removed_features.append(f\"{col1} (corr={corr:.3f} with {col2})\")\n",
        "                print(f\"  \u274c Removed: {col1} (keeping {col2})\")\n",
        "        elif 'grp' in col2.lower() and ('spend' in col1.lower() or 'cost' in col1.lower()):\n",
        "            # Keep col1 (spend), remove col2 (GRP)\n",
        "            if col2 in df_optimized.columns:\n",
        "                df_optimized = df_optimized.drop(columns=[col2])\n",
        "                removed_features.append(f\"{col2} (corr={corr:.3f} with {col1})\")\n",
        "                print(f\"  \u274c Removed: {col2} (keeping {col1})\")\n",
        "    \n",
        "    # 3. Handle other high correlations (impressions vs cost, etc.)\n",
        "    print(f\"\\n3\ufe0f\u20e3 Handling other high correlations...\")\n",
        "    for col1, col2, corr in high_corrs:\n",
        "        # Skip if already handled in GRP vs spend\n",
        "        if (col1, col2, corr) in grp_spend_corrs:\n",
        "            continue\n",
        "        \n",
        "        # For impressions vs cost, keep cost\n",
        "        if col1 in df_optimized.columns and col2 in df_optimized.columns:\n",
        "            if 'impression' in col1.lower() and 'cost' in col2.lower():\n",
        "                df_optimized = df_optimized.drop(columns=[col1])\n",
        "                removed_features.append(f\"{col1} (corr={corr:.3f} with {col2})\")\n",
        "                print(f\"  \u274c Removed: {col1} (keeping {col2})\")\n",
        "            elif 'impression' in col2.lower() and 'cost' in col1.lower():\n",
        "                df_optimized = df_optimized.drop(columns=[col2])\n",
        "                removed_features.append(f\"{col2} (corr={corr:.3f} with {col1})\")\n",
        "                print(f\"  \u274c Removed: {col2} (keeping {col1})\")\n",
        "    \n",
        "    # 3b. EDA-Informed Weather Variable Optimization (CRITICAL for ice cream!)\n",
        "    print(f\"\\n3\ufe0f\u20e3b EDA-Informed Weather Optimization...\")\n",
        "    print(f\"    \ud83d\udd0d EDA insight: sunshine_duration has 0.664 correlation - CRITICAL!\")\n",
        "    print(f\"    \ud83d\udd0d EDA insight: temperature_mean has 0.626 correlation - CRITICAL!\")\n",
        "    print(f\"    \ud83c\udf1e Weather is one of the strongest predictors for ice cream sales!\")\n",
        "    \n",
        "    # Remove only redundant temperature variables (keep mean, remove min/max)\n",
        "    weather_to_remove = ['weather_temperature_min', 'weather_temperature_max']\n",
        "    for weather_var in weather_to_remove:\n",
        "        if weather_var in df_optimized.columns:\n",
        "            df_optimized = df_optimized.drop(columns=[weather_var])\n",
        "            removed_features.append(f\"{weather_var} (redundant with temperature_mean)\")\n",
        "            print(f\"  \u274c Removed: {weather_var} (keeping temperature_mean)\")\n",
        "    \n",
        "    # KEEP the critical weather variables based on EDA\n",
        "    critical_weather = ['weather_sunshine_duration', 'weather_temperature_mean']\n",
        "    for weather_var in critical_weather:\n",
        "        if weather_var in df_optimized.columns:\n",
        "            if 'sunshine' in weather_var:\n",
        "                print(f\"  \u2705 KEPT: {weather_var} (0.664 correlation - ice cream weather!)\")\n",
        "            else:\n",
        "                print(f\"  \u2705 KEPT: {weather_var} (0.626 correlation - temperature drives sales!)\")\n",
        "    \n",
        "    # 3c. EDA-Informed granular feature handling\n",
        "    print(f\"\\n3\ufe0f\u20e3c EDA-Informed Granular Feature Review...\")\n",
        "    print(f\"    \ud83d\udd0d EDA shows is_month_end has 0.117 correlation - but too granular for MMM\")\n",
        "    \n",
        "    # Remove overly granular features\n",
        "    granular_features = ['is_month_end']\n",
        "    for feature in granular_features:\n",
        "        if feature in df_optimized.columns:\n",
        "            df_optimized = df_optimized.drop(columns=[feature])\n",
        "            removed_features.append(f\"{feature} (too granular for MMM, captured by seasonality)\")\n",
        "            print(f\"  \u274c Removed: {feature} (too granular, seasonality captured by cyclical features)\")\n",
        "    \n",
        "    # 4. EDA-Informed categorical variable handling\n",
        "    print(f\"\\n4\ufe0f\u20e3 EDA-Informed Categorical Variable Handling...\")\n",
        "    print(f\"    \ud83d\udd0d EDA shows promotions have 30% coverage - sporadic nature is normal\")\n",
        "    print(f\"    \ud83d\udd0d 3 promotion types found - manageable for MMM\")\n",
        "    \n",
        "    categorical_cols = df_optimized.select_dtypes(include=['object']).columns.tolist()\n",
        "    categorical_cols = [col for col in categorical_cols if col != 'date']\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        # For promotion type - EDA shows this is important to keep\n",
        "        if 'promo' in col.lower():\n",
        "            unique_values = df_optimized[col].nunique()\n",
        "            non_missing_count = df_optimized[col].notna().sum()\n",
        "            total_count = len(df_optimized)\n",
        "            coverage_pct = (non_missing_count / total_count) * 100\n",
        "            \n",
        "            print(f\"    \ud83d\udcca {col}: {unique_values} categories, {coverage_pct:.1f}% coverage\")\n",
        "            \n",
        "            if unique_values <= 5:  # Manageable categories\n",
        "                # Convert to dummy variables\n",
        "                dummies = pd.get_dummies(df_optimized[col], prefix=col, dummy_na=True)\n",
        "                df_optimized = df_optimized.drop(columns=[col])\n",
        "                df_optimized = pd.concat([df_optimized, dummies], axis=1)\n",
        "                print(f\"  \u2705 KEPT: {col} converted to {len(dummies.columns)} dummy variables\")\n",
        "                print(f\"    \ud83d\udd0d Promotions are important for MMM despite sparse coverage\")\n",
        "            else:\n",
        "                df_optimized = df_optimized.drop(columns=[col])\n",
        "                removed_features.append(f\"{col} (too many categories: {unique_values})\")\n",
        "                print(f\"  \u274c Removed: {col} (too many categories)\")\n",
        "        else:\n",
        "            # Remove other categorical variables\n",
        "            df_optimized = df_optimized.drop(columns=[col])\n",
        "            removed_features.append(f\"{col} (categorical - not business critical)\")\n",
        "            print(f\"  \u274c Removed: {col} (categorical)\")\n",
        "    \n",
        "    print(f\"\\n\u2705 EDA-Informed Optimization Complete:\")\n",
        "    print(f\"  \ud83d\udcca Before: {df.shape[1]} features\")\n",
        "    print(f\"  \ud83d\udcca After: {df_optimized.shape[1]} features\")\n",
        "    print(f\"  \ud83d\uddd1\ufe0f Removed: {len(removed_features)} features\")\n",
        "    print(f\"  \ud83c\udf1e PRESERVED: Critical weather variables (sunshine + temperature)\")\n",
        "    print(f\"  \ud83d\udcc5 PRESERVED: Critical seasonality (week_cos + monthly cycles)\")\n",
        "    print(f\"  \ud83d\udcb0 PRESERVED: All effective media channels\")\n",
        "    \n",
        "    return df_optimized, removed_features\n",
        "\n",
        "# Apply feature optimization to both datasets\n",
        "print(f\"\\n\ud83d\udd27 OPTIMIZING BOTH DATASETS:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "df_complete_optimized, removed_features_complete = optimize_features_for_mmm(\n",
        "    df_complete, grp_spend_corrs_complete, high_corrs_complete, \"COMPLETE CHANNELS\"\n",
        ")\n",
        "\n",
        "df_consistent_optimized, removed_features_consistent = optimize_features_for_mmm(\n",
        "    df_consistent, grp_spend_corrs_consistent, high_corrs_consistent, \"CONSISTENT CHANNELS\"\n",
        ")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Check feature-to-sample ratio\n",
        "def check_feature_sample_ratio(df):\n",
        "    \"\"\"\n",
        "    Check if we have appropriate feature-to-sample ratio for MMM\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udcca FEATURE-TO-SAMPLE RATIO CHECK\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    n_rows = len(df)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    # Exclude date and target variable\n",
        "    feature_cols = [col for col in numeric_cols if col not in ['date', 'sales']]\n",
        "    n_features = len(feature_cols)\n",
        "    \n",
        "    recommended_max_features = n_rows // 10  # Rule of thumb: 1 feature per 10 samples\n",
        "    feature_ratio = n_features / n_rows\n",
        "    \n",
        "    print(f\"\ud83d\udccf Dataset size: {n_rows} samples\")\n",
        "    print(f\"\ud83d\udd27 Current features: {n_features}\")\n",
        "    print(f\"\ud83d\udccb Recommended max features: {recommended_max_features}\")\n",
        "    print(f\"\ud83d\udcca Feature-to-sample ratio: {feature_ratio:.3f}\")\n",
        "    print(f\"\ud83d\udcc8 Status: {'\u2705 Good' if n_features <= recommended_max_features else '\u26a0\ufe0f Too many features'}\")\n",
        "    \n",
        "    if n_features > recommended_max_features:\n",
        "        excess_features = n_features - recommended_max_features\n",
        "        print(f\"\u26a0\ufe0f  Consider removing {excess_features} more features\")\n",
        "    \n",
        "    return n_features <= recommended_max_features\n",
        "\n",
        "print(f\"\\n\ud83d\udcca CHECKING FEATURE-SAMPLE RATIOS:\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Complete Channels Dataset:\")\n",
        "ratio_check_complete = check_feature_sample_ratio(df_complete_optimized)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Consistent Channels Dataset:\")\n",
        "ratio_check_consistent = check_feature_sample_ratio(df_consistent_optimized)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Temporal validation setup\n",
        "def setup_temporal_validation(df, test_months=6):\n",
        "    \"\"\"\n",
        "    Set up proper temporal validation for MMM (no data leakage!)\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udcc5 TEMPORAL VALIDATION SETUP\")\n",
        "    print(\"=\" * 35)\n",
        "    \n",
        "    if 'date' not in df.columns:\n",
        "        print(\"\u274c Date column required for temporal validation\")\n",
        "        return None\n",
        "    \n",
        "    df_sorted = df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Split data temporally\n",
        "    max_date = df_sorted['date'].max()\n",
        "    split_date = max_date - pd.DateOffset(months=test_months)\n",
        "    \n",
        "    train_df = df_sorted[df_sorted['date'] < split_date].copy()\n",
        "    test_df = df_sorted[df_sorted['date'] >= split_date].copy()\n",
        "    \n",
        "    print(f\"\ud83d\udcca Train set: {len(train_df)} samples\")\n",
        "    print(f\"  \ud83d\udcc5 Period: {train_df['date'].min().date()} to {train_df['date'].max().date()}\")\n",
        "    print(f\"\ud83d\udcca Test set: {len(test_df)} samples\")\n",
        "    print(f\"  \ud83d\udcc5 Period: {test_df['date'].min().date()} to {test_df['date'].max().date()}\")\n",
        "    \n",
        "    # Check for seasonality coverage\n",
        "    train_months = train_df['date'].dt.month.unique()\n",
        "    test_months = test_df['date'].dt.month.unique()\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd04 Seasonality coverage:\")\n",
        "    print(f\"  Train months: {sorted(train_months)}\")\n",
        "    print(f\"  Test months: {sorted(test_months)}\")\n",
        "    \n",
        "    # Rolling forecast validation setup\n",
        "    print(f\"\\n\ud83d\udd04 Rolling forecast validation periods:\")\n",
        "    validation_periods = []\n",
        "    \n",
        "    # Create multiple validation periods (holdout forecasting)\n",
        "    for months_back in [3, 6, 9, 12]:\n",
        "        if months_back <= len(df_sorted) // 4:  # Only if we have enough data\n",
        "            val_split = max_date - pd.DateOffset(months=months_back)\n",
        "            val_train = df_sorted[df_sorted['date'] < val_split]\n",
        "            val_test = df_sorted[df_sorted['date'] >= val_split]\n",
        "            \n",
        "            if len(val_train) > 20 and len(val_test) > 4:  # Minimum sizes\n",
        "                validation_periods.append({\n",
        "                    'months_back': months_back,\n",
        "                    'split_date': val_split,\n",
        "                    'train_size': len(val_train),\n",
        "                    'test_size': len(val_test)\n",
        "                })\n",
        "                print(f\"  {months_back}M back: {len(val_train)} train, {len(val_test)} test\")\n",
        "    \n",
        "    return {\n",
        "        'train_df': train_df,\n",
        "        'test_df': test_df,\n",
        "        'validation_periods': validation_periods,\n",
        "        'split_date': split_date\n",
        "    }\n",
        "\n",
        "# Set up temporal validation for both datasets\n",
        "print(f\"\\n\ud83d\udcc5 SETTING UP TEMPORAL VALIDATION:\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "print(f\"\\n\ud83d\udcc5 Complete Channels Dataset (2022-2023):\")\n",
        "validation_setup_complete = setup_temporal_validation(df_complete_optimized, test_months=3)\n",
        "\n",
        "print(f\"\\n\ud83d\udcc5 Consistent Channels Dataset (2022-2024):\")\n",
        "validation_setup_consistent = setup_temporal_validation(df_consistent_optimized, test_months=6)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Final feature summary and correlation heatmap\n",
        "def create_final_feature_summary(df):\n",
        "    \"\"\"\n",
        "    Create final summary of optimized features\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udccb FINAL FEATURE SUMMARY\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    feature_cols = [col for col in numeric_cols if col not in ['date', 'sales']]\n",
        "    \n",
        "    # Categorize features\n",
        "    feature_categories = {\n",
        "        'Time/Seasonality': [],\n",
        "        'Media Spend': [],\n",
        "        'Media Other': [],\n",
        "        'Weather': [],\n",
        "        'Promotion': [],\n",
        "        'Other': []\n",
        "    }\n",
        "    \n",
        "    for col in feature_cols:\n",
        "        col_lower = col.lower()\n",
        "        if any(x in col_lower for x in ['month_sin', 'month_cos', 'week_sin', 'week_cos', 'season', 'holiday']):\n",
        "            feature_categories['Time/Seasonality'].append(col)\n",
        "        elif any(x in col_lower for x in ['spend', 'cost']):\n",
        "            feature_categories['Media Spend'].append(col)\n",
        "        elif any(x in col_lower for x in ['grp', 'impression', 'campaign', 'click']):\n",
        "            feature_categories['Media Other'].append(col)\n",
        "        elif 'weather' in col_lower:\n",
        "            feature_categories['Weather'].append(col)\n",
        "        elif 'promo' in col_lower:\n",
        "            feature_categories['Promotion'].append(col)\n",
        "        else:\n",
        "            feature_categories['Other'].append(col)\n",
        "    \n",
        "    print(f\"\ud83c\udfaf Target variable: sales\")\n",
        "    print(f\"\ud83d\udcca Total features: {len(feature_cols)}\")\n",
        "    print()\n",
        "    \n",
        "    for category, cols in feature_categories.items():\n",
        "        if cols:\n",
        "            print(f\"\ud83d\udcc8 {category}: {len(cols)} features\")\n",
        "            for col in cols:\n",
        "                print(f\"  - {col}\")\n",
        "    \n",
        "    # Create correlation heatmap of final features\n",
        "    if len(feature_cols) > 1:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        corr_matrix = df[feature_cols].corr()\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        \n",
        "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
        "                   square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Final Feature Correlation Matrix')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return feature_categories\n",
        "\n",
        "print(f\"\\n\ud83d\udccb CREATING FEATURE SUMMARIES:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Complete Channels Dataset:\")\n",
        "feature_summary_complete = create_final_feature_summary(df_complete_optimized)\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Consistent Channels Dataset:\")\n",
        "feature_summary_consistent = create_final_feature_summary(df_consistent_optimized)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Save MMM-ready datasets\n",
        "def save_mmm_ready_datasets(df_complete, df_consistent, validation_complete, validation_consistent, \n",
        "                           removed_complete, removed_consistent, summary_complete, summary_consistent):\n",
        "    \"\"\"\n",
        "    Save both final MMM-ready datasets and configurations\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udcbe SAVING MMM-READY DATASETS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Create output directory\n",
        "    output_dir = 'data/mmm_ready'\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save both optimized datasets\n",
        "    complete_path = os.path.join(output_dir, 'mmm_complete_channels_2022_2023.csv')\n",
        "    consistent_path = os.path.join(output_dir, 'mmm_consistent_channels_2022_2024.csv')\n",
        "    \n",
        "    df_complete.to_csv(complete_path, index=False)\n",
        "    df_consistent.to_csv(consistent_path, index=False)\n",
        "    \n",
        "    print(f\"\u2705 Saved: mmm_complete_channels_2022_2023.csv ({df_complete.shape})\")\n",
        "    print(f\"  \ud83d\udce7 Includes email campaigns\")\n",
        "    print(f\"  \ud83d\udcc5 Period: 2022-2023 (2 years)\")\n",
        "    \n",
        "    print(f\"\u2705 Saved: mmm_consistent_channels_2022_2024.csv ({df_consistent.shape})\")\n",
        "    print(f\"  \ud83d\udeab Excludes email campaigns\")\n",
        "    print(f\"  \ud83d\udcc5 Period: 2022-2024 (3 years)\")\n",
        "    \n",
        "    # Save train/test splits for both datasets\n",
        "    if validation_complete:\n",
        "        complete_train_path = os.path.join(output_dir, 'complete_channels_train_set.csv')\n",
        "        complete_test_path = os.path.join(output_dir, 'complete_channels_test_set.csv')\n",
        "        \n",
        "        validation_complete['train_df'].to_csv(complete_train_path, index=False)\n",
        "        validation_complete['test_df'].to_csv(complete_test_path, index=False)\n",
        "        \n",
        "        print(f\"\u2705 Saved: complete_channels_train_set.csv ({validation_complete['train_df'].shape})\")\n",
        "        print(f\"\u2705 Saved: complete_channels_test_set.csv ({validation_complete['test_df'].shape})\")\n",
        "    \n",
        "    if validation_consistent:\n",
        "        consistent_train_path = os.path.join(output_dir, 'consistent_channels_train_set.csv')\n",
        "        consistent_test_path = os.path.join(output_dir, 'consistent_channels_test_set.csv')\n",
        "        \n",
        "        validation_consistent['train_df'].to_csv(consistent_train_path, index=False)\n",
        "        validation_consistent['test_df'].to_csv(consistent_test_path, index=False)\n",
        "        \n",
        "        print(f\"\u2705 Saved: consistent_channels_train_set.csv ({validation_consistent['train_df'].shape})\")\n",
        "        print(f\"\u2705 Saved: consistent_channels_test_set.csv ({validation_consistent['test_df'].shape})\")\n",
        "    \n",
        "    # Save optimization reports for both datasets\n",
        "    import json\n",
        "    \n",
        "    # Complete channels report\n",
        "    complete_report = {\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'dataset_type': 'complete_channels_with_email',\n",
        "        'period': '2022-2023',\n",
        "        'original_shape': df_complete.shape,\n",
        "        'optimized_shape': df_complete.shape,\n",
        "        'removed_features': removed_complete,\n",
        "        'feature_categories': summary_complete,\n",
        "        'validation_setup': {\n",
        "            'method': 'temporal_split',\n",
        "            'test_months': 3,\n",
        "            'split_date': validation_complete['split_date'].isoformat() if validation_complete else None,\n",
        "            'train_size': len(validation_complete['train_df']) if validation_complete else None,\n",
        "            'test_size': len(validation_complete['test_df']) if validation_complete else None,\n",
        "            'validation_periods': validation_complete['validation_periods'] if validation_complete else []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Consistent channels report\n",
        "    consistent_report = {\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'dataset_type': 'consistent_channels_no_email',\n",
        "        'period': '2022-2024',\n",
        "        'original_shape': df_consistent.shape,\n",
        "        'optimized_shape': df_consistent.shape,\n",
        "        'removed_features': removed_consistent,\n",
        "        'feature_categories': summary_consistent,\n",
        "        'validation_setup': {\n",
        "            'method': 'temporal_split',\n",
        "            'test_months': 6,\n",
        "            'split_date': validation_consistent['split_date'].isoformat() if validation_consistent else None,\n",
        "            'train_size': len(validation_consistent['train_df']) if validation_consistent else None,\n",
        "            'test_size': len(validation_consistent['test_df']) if validation_consistent else None,\n",
        "            'validation_periods': validation_consistent['validation_periods'] if validation_consistent else []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Combined report\n",
        "    combined_report = {\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'datasets': {\n",
        "            'complete_channels': complete_report,\n",
        "            'consistent_channels': consistent_report\n",
        "        },\n",
        "        'mmm_guidelines': {\n",
        "            'feature_sample_ratio': 'Aim for 1 feature per 10 samples',\n",
        "            'validation_method': 'Temporal cross-validation (no random splits)',\n",
        "            'avoided_issues': ['GRP-spend multicollinearity', 'data leakage', 'excess features', 'email inconsistency'],\n",
        "            'best_practices': ['Holdout forecasting', 'Seasonal coverage', 'Business logic validation']\n",
        "        },\n",
        "        'dataset_selection_guide': {\n",
        "            'complete_channels': 'Use for email effectiveness analysis and short-term insights (2022-2023)',\n",
        "            'consistent_channels': 'Use for long-term MMM and channel optimization (2022-2024)'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Convert non-serializable objects\n",
        "    def convert_for_json(obj):\n",
        "        if isinstance(obj, (pd.Timestamp, pd.DatetimeIndex)):\n",
        "            return obj.isoformat()\n",
        "        elif isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_for_json(item) for item in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: convert_for_json(value) for key, value in obj.items()}\n",
        "        else:\n",
        "            return obj\n",
        "    \n",
        "    # Save individual reports\n",
        "    complete_report_path = os.path.join(output_dir, 'mmm_complete_channels_report.json')\n",
        "    consistent_report_path = os.path.join(output_dir, 'mmm_consistent_channels_report.json')\n",
        "    combined_report_path = os.path.join(output_dir, 'mmm_optimization_report.json')\n",
        "    \n",
        "    with open(complete_report_path, 'w') as f:\n",
        "        json.dump(convert_for_json(complete_report), f, indent=2)\n",
        "    \n",
        "    with open(consistent_report_path, 'w') as f:\n",
        "        json.dump(convert_for_json(consistent_report), f, indent=2)\n",
        "        \n",
        "    with open(combined_report_path, 'w') as f:\n",
        "        json.dump(convert_for_json(combined_report), f, indent=2)\n",
        "    \n",
        "    print(f\"\u2705 Saved: mmm_complete_channels_report.json\")\n",
        "    print(f\"\u2705 Saved: mmm_consistent_channels_report.json\")\n",
        "    print(f\"\u2705 Saved: mmm_optimization_report.json\")\n",
        "    print(f\"\\n\ud83d\udcc2 All files saved to: {output_dir}\")\n",
        "\n",
        "# Save MMM-ready datasets\n",
        "save_mmm_ready_datasets(\n",
        "    df_complete_optimized, df_consistent_optimized,\n",
        "    validation_setup_complete, validation_setup_consistent,\n",
        "    removed_features_complete, removed_features_consistent,\n",
        "    feature_summary_complete, feature_summary_consistent\n",
        ")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Final recommendations\n",
        "print(f\"\\n\ud83d\ude80 DUAL MMM OPTIMIZATION COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n\u2705 FEATURE OPTIMIZATION RESULTS:\")\n",
        "print(f\"\\n\ud83d\udcca Complete Channels Dataset (2022-2023):\")\n",
        "print(f\"  \ud83d\udcca Optimized features: {df_complete_optimized.shape[1]}\")\n",
        "print(f\"  \ud83d\udccf Feature-sample ratio: {'\u2705 Good' if ratio_check_complete else '\u26a0\ufe0f Needs review'}\")\n",
        "print(f\"  \ud83d\udce7 Includes email campaigns\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Consistent Channels Dataset (2022-2024):\")\n",
        "print(f\"  \ud83d\udcca Optimized features: {df_consistent_optimized.shape[1]}\")\n",
        "print(f\"  \ud83d\udccf Feature-sample ratio: {'\u2705 Good' if ratio_check_consistent else '\u26a0\ufe0f Needs review'}\")\n",
        "print(f\"  \ud83d\udeab Excludes email campaigns\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd27 EDA-INFORMED OPTIMIZATIONS APPLIED:\")\n",
        "print(f\"  \u2705 Fixed email campaign inconsistency issue\")\n",
        "print(f\"  \u2705 PRESERVED critical weather variables (0.664 & 0.626 correlations!)\")\n",
        "print(f\"  \u2705 PRESERVED crucial seasonality (week_cos: 0.724 correlation!)\")\n",
        "print(f\"  \u2705 Removed GRP-spend perfect correlations (kept effective spend vars)\")\n",
        "print(f\"  \u2705 KEPT all media channels (EDA shows all are budget-justified)\")\n",
        "print(f\"  \u2705 Business-informed categorical handling\")\n",
        "print(f\"  \u2705 Set up temporal validation (no data leakage)\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc5 VALIDATION SETUP:\")\n",
        "print(f\"  \ud83c\udfaf Method: Temporal cross-validation\")\n",
        "print(f\"  \ud83d\udcca Complete dataset: {validation_setup_complete['train_df'].shape[0] if validation_setup_complete else 'N/A'} train, {validation_setup_complete['test_df'].shape[0] if validation_setup_complete else 'N/A'} test\")\n",
        "print(f\"  \ud83d\udcca Consistent dataset: {validation_setup_consistent['train_df'].shape[0] if validation_setup_consistent else 'N/A'} train, {validation_setup_consistent['test_df'].shape[0] if validation_setup_consistent else 'N/A'} test\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb DATASET SELECTION GUIDE:\")\n",
        "print(f\"  \ud83d\udce7 Complete Channels: Email effectiveness analysis + short-term insights\")\n",
        "print(f\"  \ud83c\udfaf Consistent Channels: Long-term MMM + channel optimization\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf NEXT STEPS:\")\n",
        "print(f\"  1. \ud83d\udcca Use EDA-optimized datasets for MMM development\")\n",
        "print(f\"  2. \ud83c\udf1e Leverage weather variables as key predictors\")\n",
        "print(f\"  3. \ud83d\udcc5 Capture seasonal cycles in MMM (crucial for ice cream!)\")\n",
        "print(f\"  4. \ud83e\udd16 Build sophisticated MMM with business-informed features\")\n",
        "print(f\"  5. \u2705 Validate against EDA insights and business logic\")\n",
        "\n",
        "print(f\"\\n\u2705 READY FOR ADVANCED MMM WITH EDA-INFORMED FEATURES!\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA-Informed MMM Feature Optimization Complete! \ud83c\udfaf\n",
        "\n",
        "### \u2705 **EDA-Informed Optimizations Applied:**\n",
        "- **Email Inconsistency Fixed**: Two datasets for different analysis needs\n",
        "- **Weather Variables PRESERVED**: Critical predictors (sunshine: 0.664, temp: 0.626)\n",
        "- **Seasonality PRESERVED**: Key predictor (week_cos: 0.724 correlation!)\n",
        "- **All Media Channels KEPT**: EDA shows budget justification and effectiveness\n",
        "- **Multicollinearity Fixed**: Removed GRP variables, kept effective spend/cost\n",
        "- **Business-Informed Decisions**: Based on ice cream sales patterns\n",
        "- **Temporal Validation**: Proper train/test splits (no data leakage)\n",
        "\n",
        "### \ud83d\udcca **Files Created:**\n",
        "- `mmm_complete_channels_2022_2023.csv` - Dataset WITH email campaigns\n",
        "- `mmm_consistent_channels_2022_2024.csv` - Dataset WITHOUT email campaigns\n",
        "- Train/test splits for both datasets\n",
        "- Individual and combined optimization reports\n",
        "\n",
        "### \ud83d\udccb **Dataset Selection Guide:**\n",
        "- **Complete Channels (2022-2023)**: Use for email effectiveness analysis\n",
        "- **Consistent Channels (2022-2024)**: Use for long-term MMM and optimization\n",
        "\n",
        "### \ud83d\ude80 **Next Phase:**\n",
        "Ready for MMM modeling with two optimized datasets and proper validation framework! \n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}